{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c8cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fd22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfn1 = 'train_data_sr_scaled_25n_'\n",
    "tfn2 = 'valid_data_sr_scaled_25n_'\n",
    "\n",
    "nsets = 3\n",
    "model_file_name = 'sr_scaled_5f_t25_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfd2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = pd.read_csv('../src/data/raw/train.csv')\n",
    "tdata['name'] = tdata['name'].isna().astype(int)\n",
    "tdata['categories'] = tdata['categories'].isna().astype(int)\n",
    "tdata['address'] = tdata['address'].isna().astype(int)\n",
    "tdata['state'] = tdata['state'].isna().astype(int)\n",
    "tdata['url'] = tdata['url'].isna().astype(int)\n",
    "tdata['country'] = tdata['country'].isna().astype(int)\n",
    "\n",
    "tdata = tdata[['id','name', 'categories', 'address', 'state', \n",
    "               'url', 'country']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9d25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_set):\n",
    "    \n",
    "    train_df = []\n",
    "    \n",
    "    for i in range(1, nsets+1):\n",
    "    \n",
    "        t1 = pd.read_csv('../src/data/processed/'+df_set+str(i)+'.csv')\n",
    "        print(t1.shape)\n",
    "        \n",
    "        t1['fuzz_power_all'] = 1-t1[[col for col in t1.columns if 'fuzz_power' \n",
    "                           in col]].sum(axis=1)/(100*len([col for col in t1.columns if 'fuzz_power' in col]))\n",
    "        t1['sim_mean'] = 1 - t1[[col for col in t1.columns if 'sim' in col]].mean(axis=1)\n",
    "\n",
    "#         t1['kdist_diff'] = (t1['kdist'] - t1['kdist_country'])/t1['kdist_country']\n",
    "\n",
    "        t1['kdist_diff'] = ((t1['kdist'].fillna(0) + t1['fuzz_power_all'] + t1['sim_mean']) \n",
    "                            - (t1['kdist_country'].fillna(0) + t1['fuzz_power_all'] + \n",
    "                              t1['sim_mean']))/(t1['kdist_country'].fillna(0) + t1['fuzz_power_all'] + t1['sim_mean'])\n",
    "        \n",
    "        t1 = t1.drop('fuzz_power_all', axis=1)\n",
    "        \n",
    "        t1['kneighbors_mean'] = t1[['kneighbors', 'kneighbors_country']].mean(axis = 1)\n",
    "        \n",
    "        t1['sim_mean'] = t1[[col for col in t1.columns if 'sim' in col]].mean(axis=1)\n",
    "        t1['jaro_mean'] = t1[[col for col in t1.columns if 'jaro' in col]].mean(axis=1)\n",
    "        t1['lcs_mean'] = t1[[col for col in t1.columns if '_lcs' in col]].mean(axis=1)        \n",
    "        \n",
    "        t1['sim_sum'] = t1[[col for col in t1.columns if 'sim' in col]].sum(axis=1)\n",
    "        t1['gesh_sum'] = t1[[col for col in t1.columns if 'gesh' in col]].sum(axis=1)\n",
    "        t1['leven_sum'] = t1[[col for col in t1.columns if '_leven' in col]].sum(axis=1)\n",
    "        t1['jaro_sum'] = t1[[col for col in t1.columns if 'jaro' in col]].sum(axis=1)\n",
    "        t1['lcs_sum'] = t1[[col for col in t1.columns if '_lcs' in col]].sum(axis=1)\n",
    "        t1['nlcsk_sum'] = t1[[col for col in t1.columns if '_nlcsk' in col]].sum(axis=1)\n",
    "        t1['nleven_sum'] = t1[[col for col in t1.columns if '_nleven' in col]].sum(axis=1)\n",
    "        t1['nlcs_sum'] = t1[[col for col in t1.columns if '_nlcs' in col]].sum(axis=1)\n",
    "\n",
    "        t1['sim_std'] = t1[[col for col in t1.columns if 'sim' in col]].std(axis=1)\n",
    "        t1['gesh_std'] = t1[[col for col in t1.columns if 'gesh' in col]].std(axis=1)\n",
    "        t1['leven_std'] = t1[[col for col in t1.columns if '_leven' in col]].std(axis=1)\n",
    "        t1['jaro_std'] = t1[[col for col in t1.columns if 'jaro' in col]].std(axis=1)\n",
    "        t1['lcs_std'] = t1[[col for col in t1.columns if '_lcs' in col]].std(axis=1)\n",
    "        t1['nlcsk_std'] = t1[[col for col in t1.columns if '_nlcsk' in col]].std(axis=1)\n",
    "        t1['nleven_std'] = t1[[col for col in t1.columns if '_nleven' in col]].std(axis=1)\n",
    "        t1['nlcs_std'] = t1[[col for col in t1.columns if '_nlcs' in col]].std(axis=1)\n",
    "\n",
    "        t1 = t1.merge(tdata, on='id', how='left')\n",
    "        t1 = t1.merge(tdata, left_on='match_id', right_on='id', how='left', suffixes=['_1','_2'])\n",
    "        t1 = t1.drop('id_2', axis=1).rename(columns={'id_1':'id'})\n",
    "        \n",
    "        t1['info_power_1'] = t1[[col for col in t1.columns if '_1' in col]].lt(1).sum(axis=1)\n",
    "        t1['info_power_2'] = t1[[col for col in t1.columns if '_2' in col]].lt(1).sum(axis=1)\n",
    "        t1['info_diff'] = t1['info_power_1'] - t1['info_power_2']\n",
    "        \n",
    "        t1['kdist_diff_x_info_diff'] = t1['kdist_diff']*t1['info_diff']\n",
    "        \n",
    "        t1 = t1.drop([col for col in t1.columns if '_1' in col], axis=1)\n",
    "        t1 = t1.drop([col for col in t1.columns if '_2' in col], axis=1)\n",
    "        gc.collect()\n",
    "        \n",
    "        train_df.append(t1)\n",
    "        \n",
    "        del t1\n",
    "        gc.collect()\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d87800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5511558, 92)\n",
      "(5498470, 92)\n",
      "(5522210, 92)\n",
      "CPU times: user 6min 52s, sys: 2min 37s, total: 9min 30s\n",
      "Wall time: 9min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = prepare_data(tfn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68adf156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 7.65 s, total: 20.1 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.concat(train_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8184a3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16532238, 115)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21cd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5493835, 92)\n",
      "(5523338, 92)\n",
      "(5505856, 92)\n",
      "CPU times: user 6min 52s, sys: 2min 38s, total: 9min 31s\n",
      "Wall time: 9min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_df = prepare_data(tfn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb09384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 12 s, total: 27.9 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_df = pd.concat(valid_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b852379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16523029, 115)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40dd4e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15501174\n",
       "1     1031064\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f62da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15499140\n",
       "1     1023889\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc249e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.2 s, sys: 38.5 s, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.concat([train_df, valid_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "del valid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958180f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.937833\n",
       "1    0.062167\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81cbd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FEATURES = ['kdist', 'kneighbors', 'kdist_country', 'kneighbors_country', 'name_sim', 'name_gesh','name_leven', \n",
    "                'name_jaro', 'name_lcs', 'name_len_diff', 'name_nleven', 'name_nlcsk', 'name_nlcs', 'address_sim', \n",
    "                'address_gesh', 'address_leven', 'address_jaro', 'address_lcs', 'address_len_diff', 'address_nleven', \n",
    "                'address_nlcsk', 'address_nlcs', 'city_gesh', 'city_leven', 'city_jaro', 'city_lcs', 'city_len_diff', \n",
    "                'city_nleven', 'city_nlcsk', 'city_nlcs', 'state_sim', 'state_gesh', 'state_leven', 'state_jaro', \n",
    "                'state_lcs', 'state_len_diff', 'state_nleven', 'state_nlcsk', 'state_nlcs', 'zip_gesh', 'zip_leven', \n",
    "                'zip_jaro', 'zip_lcs', 'url_sim', 'url_gesh', 'url_leven', 'url_jaro', 'url_lcs', 'url_len_diff', \n",
    "                'url_nleven', 'url_nlcsk', 'url_nlcs', 'phone_gesh', 'phone_leven', 'phone_jaro', 'phone_lcs', \n",
    "                'categories_sim', 'categories_gesh', 'categories_leven', 'categories_jaro', 'categories_lcs', \n",
    "                'categories_len_diff', 'categories_nleven', 'categories_nlcsk', 'categories_nlcs', 'country_sim', \n",
    "                'country_gesh', 'country_leven', 'country_nleven', 'kdist_diff', 'kneighbors_mean', \n",
    "                'sim_sum', 'gesh_sum', 'leven_sum', 'jaro_sum','lcs_sum', 'sim_std', 'gesh_std', 'leven_std',\n",
    "                'jaro_std', 'lcs_std', 'info_diff', 'nleven_sum', 'nlcsk_sum', 'nlcs_sum', 'nleven_std', \n",
    "                'nlcsk_std', 'nlcs_std', 'sim_mean','jaro_mean','lcs_mean','kdist_diff_x_info_diff',\n",
    "                'name_w_ratio', 'name_partial_ratio', 'name_tokenset_ratio', 'name_tokensort_ratio', \n",
    "                'name_fuzz_power', 'categories_w_ratio', 'categories_partial_ratio', 'categories_tokenset_ratio', \n",
    "                'categories_tokensort_ratio', 'categories_fuzz_power', 'address_w_ratio', 'address_partial_ratio', \n",
    "                'address_tokenset_ratio', 'address_tokensort_ratio', 'address_fuzz_power'\n",
    "                ]\n",
    "\n",
    "len(TRAIN_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09dfdf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1165dca9ff3f46f981117b5703ac4b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 s, sys: 31 s, total: 1min 18s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NFOLDS = 5\n",
    "kf = StratifiedKFold(n_splits = NFOLDS, shuffle=True, random_state=42)\n",
    "for i, (trn_idx, val_idx) in tqdm(enumerate(kf.split(train_df, train_df[\"label\"], train_df[\"label\"]))):\n",
    "    train_df.loc[val_idx, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd82fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "def fit_lgbm(X, y, params=None, es_rounds=20, seed=42, N_SPLITS=5, \n",
    "             n_class=None, model_dir=None, folds=None):\n",
    "    \n",
    "    models = []\n",
    "    oof = np.zeros((len(y), n_class), dtype=np.float64)\n",
    "    \n",
    "    for i in tqdm(range(NFOLDS)):\n",
    "        print(f\"== fold {i} ==\")\n",
    "        trn_idx = folds!=i\n",
    "        val_idx = folds==i\n",
    "        X_train, y_train = X[trn_idx], y.iloc[trn_idx]\n",
    "        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        if model_dir is None:\n",
    "            model = lgbm.LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train, y_train, \n",
    "                eval_set=[(X_valid, y_valid)],  \n",
    "                early_stopping_rounds=es_rounds, \n",
    "                verbose=50)\n",
    "        else:\n",
    "            with open(f'../src/models/ScaledModels/{model_file_name}{i}.pkl', 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "        pred = model.predict_proba(X_valid)\n",
    "        oof[val_idx] = pred\n",
    "        models.append(model)\n",
    "        \n",
    "        file = f'../src/models/ScaledModels/{model_file_name}{i}.pkl'\n",
    "        pickle.dump(model, open(file, 'wb'))\n",
    "        print()\n",
    "\n",
    "    cv = (oof.argmax(axis=-1) == y).mean()\n",
    "    print(f\"CV-accuracy: {cv}\")\n",
    "\n",
    "    return oof, models\n",
    "\n",
    "def inference_lgbm(models, feat_df):\n",
    "    pred = np.array([model.predict_proba(feat_df) for model in models])\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3e26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a715f67b34f9081e7ed56470502af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== fold 0 ==\n",
      "[50]\tvalid_0's binary_logloss: 0.0353404\tvalid_0's auc: 0.994455\n",
      "[100]\tvalid_0's binary_logloss: 0.0336409\tvalid_0's auc: 0.995025\n",
      "[150]\tvalid_0's binary_logloss: 0.0328114\tvalid_0's auc: 0.99531\n",
      "[200]\tvalid_0's binary_logloss: 0.0321415\tvalid_0's auc: 0.995511\n",
      "[250]\tvalid_0's binary_logloss: 0.0316133\tvalid_0's auc: 0.995686\n",
      "[300]\tvalid_0's binary_logloss: 0.0311351\tvalid_0's auc: 0.995839\n",
      "[350]\tvalid_0's binary_logloss: 0.0306991\tvalid_0's auc: 0.995971\n"
     ]
    }
   ],
   "source": [
    "# params = {\n",
    "#     'boosting_type': 'gbdt', \n",
    "#     'objective': 'binary', \n",
    "#     'tree_learner': 'feature',\n",
    "#     'metric': ['binary_logloss', 'AUC', 'average_precision'],\n",
    "#     'learning_rate': 0.3,\n",
    "#     'reg_alpha': 0.2876,\n",
    "#     'reg_lambda': 0.882,\n",
    "#     'random_state': 2018,\n",
    "#     'max_depth': 9,\n",
    "#     'num_leaves': 127, \n",
    "#     'n_estimators': 3000,\n",
    "#     \"colsample_bytree\": 0.785,\n",
    "#     \"first_metric_only\": True,\n",
    "#     \"max_bin\": 512,\n",
    "#     \"bagging_fraction\": 0.28,\n",
    "#     \"bagging_freq\": 21,\n",
    "#     \"force_row_wise\": True\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'binary', \n",
    "    'tree_learner': 'feature',\n",
    "    'metric': ['binary_logloss', 'AUC'],\n",
    "    'learning_rate': 0.3,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.682,\n",
    "    'random_state': 2018,\n",
    "    'max_depth': 9,\n",
    "    'num_leaves': 127, \n",
    "    'n_estimators': 3000,\n",
    "    \"colsample_bytree\": 0.785,\n",
    "    \"first_metric_only\": True,\n",
    "    \"max_bin\": 512\n",
    "}\n",
    "\n",
    "oof, models = fit_lgbm(train_df[TRAIN_FEATURES], train_df[\"label\"].astype(int), \n",
    "                       params=params, n_class=int(train_df[\"label\"].max() + 1), \n",
    "                       N_SPLITS=NFOLDS, folds=train_df[\"fold\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac917ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importances(models):\n",
    "    importance_df = pd.DataFrame(models[0].feature_importances_, \n",
    "                                 index=TRAIN_FEATURES, \n",
    "                                 columns=['importance'])\\\n",
    "                        .sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.subplots(figsize=(len(TRAIN_FEATURES) // 4, 5))\n",
    "    plt.bar(importance_df.index, importance_df.importance)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_importances(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets init -p ../src/models/ScaledModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets create -p ../src/models/ScaledModels --dir-mode zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -m \"SRScaledFuzz\" -p ../src/models/ScaledModels --dir-mode zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad06a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
